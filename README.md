# Autonomous Agent for Remote Desktop Control

An autonomous AI agent with multi-modal capabilities (vision + voice + chat) that controls a remote Linux desktop environment through natural language. Built with Claude's Computer Use API, Deepgram live transcription, and E2B Desktop Sandbox.

## Features

- ğŸ¤– **Fully autonomous agentic loops** - Agent perceives, reasons, acts, and adapts based on visual feedback
- ğŸ¤ **Multi-modal input** - Voice (via Deepgram) and text chat interfaces
- ğŸ‘ï¸ **Vision-powered control** - Agent analyzes screenshots to plan and execute actions
- ğŸ–±ï¸ **Computer use tools** - Mouse clicks, keyboard input, bash commands, file editing
- ğŸ–¥ï¸ **Real-time desktop streaming** - Live Linux desktop (Xfce) streamed to browser via VNC
- ğŸ”„ **Persistent sessions** - Reconnect to existing sandbox sessions
- ğŸ“‹ **Clipboard integration** - Read/write clipboard access
- âš¡ **Streaming responses** - Real-time agent reasoning and action updates

## Architecture

The application implements a complete autonomous agent system with perception-action loops:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          User Input Layer                           â”‚
â”‚  Voice Input â”€â”€â–º Deepgram â”€â”€â–º WebSocket â”€â”€â–º Live Transcription      â”‚
â”‚  Text Input  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Chat Interface         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Agent Orchestration Layer                      â”‚
â”‚  Next.js API Routes + Server-Sent Events (SSE) Streaming            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Agentic Loop (Claude)                         â”‚
â”‚                                                                     â”‚
â”‚  1. Perception:   Take screenshot of desktop                        â”‚
â”‚  2. Reasoning:    Analyze visual state + user intent                â”‚
â”‚  3. Planning:     Decide which tool(s) to use                       â”‚
â”‚  4. Action:       Execute computer/bash/editor tools                â”‚
â”‚  5. Feedback:     Capture new screenshot                            â”‚
â”‚  6. Iterate:      Loop until task complete                          â”‚
â”‚                                                                     â”‚
â”‚  Tools: computer_use (mouse/keyboard), bash, text_editor            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Desktop Execution Layer                          â”‚
â”‚  E2B Desktop Sandbox - Isolated Linux VM with VNC streaming         â”‚
â”‚  â€¢ Resolution scaling for Claude's vision API                       â”‚
â”‚  â€¢ Action executor (clicks, typing, scrolling, bash)                â”‚
â”‚  â€¢ Screenshot capture and base64 encoding                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

- **Voice Pipeline**: Browser MediaRecorder â†’ WebSocket â†’ Deepgram Live API â†’ Real-time transcript
- **Agent Provider**: Claude Agent with Computer Use API (beta 2025-01-24)
- **Action Executor**: Translates agent decisions into desktop interactions
- **Resolution Scaler**: Adapts between display resolution and Claude's vision constraints
- **Streaming Protocol**: SSE for real-time agent reasoning, actions, and status updates

## How the Agent Works

The agent operates in a continuous perception-action loop:

1. **User sends command** (voice or text) - Natural language instruction
2. **Agent initializes sandbox** - Spins up isolated Linux VM if needed
3. **Agentic loop begins**:
    - Agent takes screenshot of desktop
    - Claude analyzes visual state and user intent
    - Plans which computer use tools to invoke
    - Executes actions (mouse clicks, typing, bash commands)
    - Takes new screenshot to verify results
    - Reasons about next steps
4. **Loop continues** until task is complete or user intervenes
5. **Desktop streams live** - User watches agent work in real-time via VNC iframe

## Screenshots

### Home Page

![Home Page](public/home-page.png)

### ğŸˆ Who's performing at the Super Bowl halftime show in 2026?

![Super Bowl Search](public/google-search.png)

### ğŸ›’ Find highly-rated dog toys on Amazon under $30

![Amazon Search](public/amazon-search.png)

## Technical Stack

- **Frontend**: Next.js
- **Agent**: Claude Agent with Computer Use tools
- **Voice**: Deepgram live transcription
- **Sandbox**: E2B Desktop Sandbox (isolated Linux VM with VNC)
- **Streaming**: WebSocket (voice), Server-Sent Events (agent responses)
- **Tools**: Computer use, Bash execution, Text editor

## Prerequisites

- Node.js 20+
- E2B API key ([get one here](https://e2b.dev))
- Anthropic API key ([get one here](https://console.anthropic.com))
- Deepgram API key ([get one here](https://deepgram.com))

## Setup

1. **Clone and install dependencies**

```bash
bun install
```

2. **Configure environment variables**

Create `.env.local` from `env.example`:

```bash
cp env.example .env.local
```

Add your API keys:

```
E2B_API_KEY=your_e2b_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
DEEPGRAM_API_KEY=your_deepgram_key_here
```

3. **Run the development server**

```bash
bun dev
```

Open [http://localhost:3000](http://localhost:3000) and start chatting or speaking to control the desktop.

## Learn More

- [E2B Desktop Documentation](https://e2b.dev/docs/template/examples/desktop)
- [Anthropic Computer Use](https://docs.anthropic.com/en/docs/agents/computer-use)
- [Deepgram Live Transcription](https://developers.deepgram.com/docs/streaming-live-transcription)
- [Next.js Documentation](https://nextjs.org/docs)
